---
title: "Learning both position and object identity"
collection: talks
type: "Poster"
permalink: /talks/learning-both-position-and
venue: "CoSyne"
date: 2020-02-28
location: "Denver, CO"
excerpt: The ability to extract behaviorally relevant information from our environment in real-time is a key function of the visual system. Primates recognize objects and their locations in space through visual processing, which occurs in visual cortex, and they use both pieces of information to navigate their environments. Visual cortex reformats spatially organized photoreceptor potentials, relayed to cortex by the thalamus, into identity-oriented representations in inferior temporal cortex (IT) and location-oriented representations in posterior parietal cortex. What is the functional significance of separating “what” vs “where” information?
citation: '<strong>Elijah Christensen</strong> Joel Zylberberg. (2019)'
---


## Summary

The ability to extract behaviorally relevant information from our environment in real-time is a key function of the visual system. Primates recognize objects and their locations in space through visual processing, which occurs in visual cortex, and they use both pieces of information to navigate their environments. Visual cortex reformats spatially organized photoreceptor potentials, relayed to cortex by the thalamus, into identity-oriented representations in inferior temporal cortex (IT) and location-oriented representations in posterior parietal cortex. What is the functional significance of separating “what” vs “where” information? Our best performing computational models of visual perception – deep object recognition networks – explicitly throw out all spatial information through max-pooling operations [1] so they cannot reveal how or why the brain partitions spatial and identity information.  To fill this gap, we expanded upon prior work that used machine learning models called “semisupervised autoencoders”. These models simultaneously categorize and reconstruct images and learn representations of category and within-category shape variations [2,3]. We trained similar models on images that had large spatial variations in the object locations and found that in our network model some units represented object identity, while others represented object location. This suggests that in environments with variable object locations, learning to concurrently categorize objects and reconstruct visual scenes suffices to form the kinds of separate “what” and “where” representations we see in the primate visual system.

## Additional Detail

Prior work used artificial neural network (ANN) autoencoders to concurrently learn stylistic variations and category labels [2,3]. We modified the MNIST and fashion MNIST datasets to introduce spatial variations by applying random horizontal and vertical translations and rotations to each image in the dataset (Fig 1A). We aimed to find models that learn the object’s category, as well as any other information needed to reconstruct the image. Our hypothesis was that this “other information” would primarily be about the object’s position. In our model the encoder network (denoted F) with parameters ψ learns a function which will map the input image (x) that comes from category (y) onto two latent vector spaces (y ̂,z) that represent high-level abstractions like those in IT and posterior parietal cortex (Fig 1B).  The decoder network (denoted G) with parameters ϕ learns a function which uses the latent representation (y ̂,z) to draw a reconstruction (x ̂) of the original input (Fig 1B).  Mathematically, we define F and G respectively as:

Traditionally, autoencoders optimize the parameters of F and G (ψ, ϕ) to minimize the squared error between the original input and the reconstruction (〖Recon〗_error in Eq. 3). Similar to Ref. 2, we also designate part of the latent space to represent the image’s category label or class (y ̂). This is accomplished by minimizing the cross-entropy (XEnt in Eq. 3) between the network’s internal y ̂   variable, and the true class label, y, which is represented by one-hot vector. Finally, we also apply a cross-covariance penalty (XCov in Eq. 3) on each mini-batch to encourage disentangled and independent latent representations (y ̂,z) as in [2].  Notably, the only constraints on the z latent space are that it: 1) it must represent different information than y ̂; and 2) that information must be useful for reconstructing the original image. The final loss function (Eq. 3) is the sum of reconstruction error, categorical cross-entropy, and cross-covariance each weighted by α, β,and  γ.

5% of the training data was held-out and used as a validation dataset to stop training before overfitting. The loss function (Eq. 3) was minimized using mini-batch gradient descent with a momentum optimizer (ADAM) until convergence.

The network model achieves classification performance comparable to other published results when trained on location-shifted MNIST and fashion MNIST: 95% and 72% respectively.  When trained on the unshifted MNIST dataset, Ref. 2 reported the non-categorical latent space (z) represents stylistic variations within each category. Our location-shifted inputs force the network to learn representations of spatial location, in order to get accurate reconstructions. As a result, the z latent space ends up representing spatial locations (Fig 2A).
In addition to the results shown here, we will present the following at the conference: 1) Characterization of the latent representation under different amounts of spatial variation in the input images, to reveal the transition point where the latent variable z switches from representing style to space and 2) Interpolations through different regions of the latent spaces to demonstrate smoothness in the spatial representations (z).
